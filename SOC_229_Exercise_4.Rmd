---
title: "SOC_229_Exercise_4"
author: "Phoebe Jiang"
date: "2025-04-27"
output: html_document
---


```{r}
library(kernelTools)
library(caret)
data <- read.csv("C:/Users/saran/OneDrive/Documents/SOC 229/housing.csv")
set.seed(123)
idx <- sample(nrow(data), 1000)
dat <- data[idx, ]

X <- as.matrix(dat[, c("median_income", "housing_median_age",
                      "total_rooms", "total_bedrooms",
                      "population", "households",
                      "latitude", "longitude")])
y <- dat$median_house_value
cc <- complete.cases(X, y)
X <- X[cc, ];  y <- y[cc]

X_scaled <- scale(X)
split    <- createDataPartition(y, p = .75, list = FALSE)
x_tr     <- X_scaled[split, ]
y_tr     <- y[split]
x_val    <- X_scaled[-split, ]
y_val    <- y[-split]
```

```{r rbf}



```



```{r}
# ----------------------------
# 1. Comparing Flexible Kernels
# ----------------------------

# Hyperparameter grids, keyed by kernel name
param_list <- list(
  linear  = NULL,
  rbf     = list(sigma = c(0.1, 1, 10)),
  ratquad = list(alpha = c(0.5, 1), sigma = c(0.1, 1)),
  arccos  = list(order = c(1, 2)),
  nnet    = list(scale = c(0.1, 1, 10))
)

kernels <- names(param_list)
results <- list()
for (kern in kernels) {
  kp <- param_list[[kern]]
  
  if (is.null(kp)) {
    
    tuned <- kernWLSBW(
      x         = list(x_tr),
      y         = y_tr,
      kernel    = kern,
      reg.param = c(0.001, 0.01, 0.1),
      k         = 5,
      reps      = 1
    )
    best_model <- kernWLS(
      x         = list(x_tr),
      y         = y_tr,
      kernel    = kern,
      reg.param = tuned$best.reg.param
    )
    final_params <- list()
    
  } else {
    # — flexible kernels
    tuned <- kernWLSBW(
      x          = list(x_tr),
      y          = y_tr,
      kernel     = kern,
      kern.param = kp,
      reg.param  = c(0.001, 0.01, 0.1),
      k          = 5,
      reps       = 1
    )
    
    args <- c(
      list(x      = list(x_tr),
          y      = y_tr,
          kernel = kern),
      tuned[c("best.kernel.param","best.reg.param")]
    )
    best_model    <- do.call(kernWLS, args)
    final_params  <- args$best.kernel.param

    
    # coerce to a plain named list, handling both atomic vectors and 1‐row data.frames:
    if (is.data.frame(tuned$best.kernel.param)) {
      final_params <- as.list(tuned$best.kernel.param[1, , drop=TRUE])
    } else {
      final_params <- as.list(tuned$best.kernel.param)
    }
    
    best_model <- kernWLS(
      x          = list(x_tr),
      y          = y_tr,
      kernel     = kern,
      kern.param = final_params,
      reg.param  = tuned$best.reg.param
    )
  }  
  
  # predict & record MSE
  y_pred <- predict(best_model, newx = list(x_val))
  mse    <- mean((y_val - y_pred)^2)
  
  results[[kern]] <- list(
    mse    = mse,
    params = final_params
  )
}


# Results summary
for (kern in kernels) {
  cat(sprintf(
    "%-8s |  MSE = %.4f  |  best params: %s\n",
    kern,
    results[[kern]]$mse,
    if (length(results[[kern]]$params))
      paste(names(results[[kern]]$params),
            results[[kern]]$params, sep = "=", collapse = ", ")
    else
      "— linear (no params)"
  ))
}

```

```{r}
quick_kernels <- c("linear","rbf","ratquad")
results_quick <- list()

for (kern in quick_kernels) {
  kp <- param_list[[kern]]
  
  if (is.null(kp)) {
    # linear
    tuned <- kernWLSBW(
      x         = list(x_tr),
      y         = y_tr,
      kernel    = kern,
      reg.param = c(0.001, 0.01, 0.1),
      k         = 5,
      reps      = 1
    )
    best_model   <- kernWLS(
      x         = list(x_tr),
      y         = y_tr,
      kernel    = kern,
      reg.param = tuned$best.reg.param
    )
    final_params <- list()
    
  } else {
    # rbf or ratquad
    tuned <- kernWLSBW(
      x          = list(x_tr),
      y          = y_tr,
      kernel     = kern,
      kern.param = kp,
      reg.param  = c(0.001, 0.01, 0.1),
      k          = 5,
      reps       = 1
    )
    # coerce to named list
    final_params <- as.list(unlist(tuned$best.kernel.param))
    best_model   <- kernWLS(
      x          = list(x_tr),
      y          = y_tr,
      kernel     = kern,
      kern.param = final_params,
      reg.param  = tuned$best.reg.param
    )
  }
  
  # now mse exists
  y_pred <- predict(best_model, newx = list(x_val))
  mse    <- mean((y_val - y_pred)^2)
  
  results_quick[[kern]] <- list(
    mse    = mse,
    params = final_params
  )
}

print(results_quick)

```

```{r}
# 2) Then do arccos and nnet in their own loop
hard_kernels <- c("arccos","nnet")
results_hard <- list()

for (kern in hard_kernels) {
  kp <- param_list[[kern]]
  
  tuned <- kernWLSBW(
    x          = list(x_tr),
    y          = y_tr,
    kernel     = kern,
    kern.param = kp,
    reg.param  = c(0.001, 0.01, 0.1),
    k          = 5,
    reps       = 1
  )
  
  # coerce best.kernel.param to named list
  best_kp_raw <- tuned$best.kernel.param
  if (is.data.frame(best_kp_raw)) {
    final_params <- as.list(best_kp_raw[1, , drop=TRUE])
  } else {
    final_params <- as.list(best_kp_raw)
  }
  
  best_model <- kernWLS(
    x          = list(x_tr),
    y          = y_tr,
    kernel     = kern,
    kern.param = final_params,
    reg.param  = tuned$best.reg.param
  )
  
  y_pred <- predict(best_model, newx = list(x_val))
  mse    <- mean((y_val - y_pred)^2)
  
  results_hard[[kern]] <- list(mse = mse, params = final_params)
  
}

print(results_hard)

```



```{r}
# ----------------------------------
# 2. Extrapolation with Flexible Kernels
# ----------------------------------

# Select predictor (e.g., first column)
x1 <- x[,1]
q25 <- quantile(x1, 0.25)
holdout <- which(x1 <= q25)

# Split data
x_train_extrap <- x[-holdout, ]
y_train_extrap <- y[-holdout]
x_test_extrap <- x[holdout, ]
y_test_extrap <- y[holdout]

# Tune and evaluate (example with RBF kernel)
tuned_rbf <- kernWLSBW(
  x = x_train_extrap,
  y = y_train_extrap,
  kernel = "rbf",
  kern.param = list(sigma = c(0.1, 1, 10)),
  reg.param = c(0.001, 0.01, 0.1),
  k = 5
)

best_rbf <- kernWLS(
  x = x_train_extrap,
  y = y_train_extrap,
  kernel = "rbf",
  kern.param = tuned_rbf$best.kernel.param,
  reg.param = tuned_rbf$best.reg.param
)

y_pred_extrap <- predict(best_rbf, newx = x_test_extrap)
mse_extrap <- mean((y_test_extrap - y_pred_extrap)^2)
cat(sprintf("RBF Extrapolation MSE: %.4f\n", mse_extrap))

# Repeat for other kernels (ratquad, arccos, ipolyrbf, etc.)
```


```{r}
# ----------------------------------
# 3. Composite Kernel (Linear + RBF)
# ----------------------------------

# Define composite kernel (linear + RBF)
x_comp <- list(x_train, x_train)  # Same predictors for both kernels

# Tune parameters
tuned_comp <- kernWLSBW(
  x = x_comp,
  y = y_train,
  kernel = c("linear", "rbf"),
  kern.param = list(
    list(NULL),  # Linear (no parameters)
    list(sigma = c(0.1, 1, 10))  # RBF
  ),
  reg.param = c(0.001, 0.01, 0.1),
  k = 5
)

# Train final model
best_comp <- kernWLS(
  x = x_comp,
  y = y_train,
  kernel = c("linear", "rbf"),
  kern.param = tuned_comp$best.kernel.param,
  reg.param = tuned_comp$best.reg.param
)

# Predict on validation
y_pred_comp <- predict(best_comp, newx = list(x_val, x_val))
mse_comp <- mean((y_val - y_pred_comp)^2)
cat(sprintf("Composite Kernel (Linear + RBF) MSE: %.4f\n", mse_comp))
```


